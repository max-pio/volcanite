#version 460
#extension GL_GOOGLE_include_directive : require
#extension GL_EXT_debug_printf : enable
#extension GL_EXT_buffer_reference_uvec2 : require
#extension GL_EXT_buffer_reference2 : require
#extension GL_EXT_shader_explicit_arithmetic_types_int64 : require
#extension GL_KHR_shader_subgroup_basic : require

#ifndef SUBGROUP_SIZE
    #define SUBGROUP_SIZE 64
#endif

// TODO: instead of invoking brick_idx_count * SUBGROUP_SIZE for .x, invoke .x=SUBGROUP_SIZE, .y=brick_idx_count?
// brick_idx_count * SUBGROUP_SIZE invocations: one subgroup is responsible for decompressing one brick
layout (local_size_x = SUBGROUP_SIZE, local_size_y = 1, local_size_z = 1) in;


#include "volcanite/renderer/csgv_bindings.glsl"

#include "util.glsl"
#include "volcanite/compression/csgv_utils.glsl"
#define CSGV_DECODING_ARRAY g_cache


// if DECODE_FROM_SHARED_MEMORY is set, the encoding is copied to shared memory before decoding
#ifdef DECODE_FROM_SHARED_MEMORY

    #if ENCODING_MODE == NIBBLE_ENC
        #ifndef MAX_BRICK_ENCODING_32BIT_LENGTH
            #define MAX_BRICK_ENCODING_32BIT_LENGTH 12288
        #endif
        shared uint[MAX_BRICK_ENCODING_32BIT_LENGTH] s_brick_encoding;
        #define SHARED_BRICK_ENCODING s_brick_encoding
    #elif ENCODING_MODE == HUFFMAN_WM_ENC
        #include "volcanite/compression/decoder/HuffmanWMDecoder_types.glsl"

        #ifndef MAX_BIT_VECTOR_WORD_LENGTH
            #define MAX_BIT_VECTOR_WORD_LENGTH 1024
        #endif
        #define MAX_FLAT_RANK_WORD_LENGTH (BV_WORD_BIT_SIZE * MAX_BIT_VECTOR_WORD_LENGTH / BV_L1_BIT_SIZE + 1)
        // each WMHBrickHeader is 12x4x8=384 bits in size: allocate enough WMHBrickHeader entries to fit any flat rank at the end
        shared BV_WORD_TYPE s_bit_vector[MAX_BIT_VECTOR_WORD_LENGTH];
        #define MAX_WM_HEADER_ARRAY_LENGTH (2 + (MAX_FLAT_RANK_WORD_LENGTH / ((384 + BV_WORD_BIT_SIZE - 1) / BV_WORD_BIT_SIZE)))
        shared WMHBrickHeader s_wm_header[MAX_WM_HEADER_ARRAY_LENGTH];
        #define SHARED_BIT_VECTOR s_bit_vector
        #define SHARED_WM_HEADER s_wm_header[0]
        shared uvec4 s_stop_bits_ref;
        #define SHARED_STOP_BITS_REF s_stop_bits_ref
    #else
        #error "unsupported encoding mode"
    #endif

#endif

#include "volcanite/compression/compressed_segmentation_volume.glsl"

void main() {
#if CACHE_MODE == CACHE_BRICKS

    assert(gl_NumSubgroups == 1, "decompression assumes exactly one subgroup per workgroup");
    assert(gl_SubgroupSize == SUBGROUP_SIZE && gl_WorkGroupSize.x == SUBGROUP_SIZE && gl_WorkGroupSize.y == 1u
            && gl_WorkGroupSize.z == 1u, "decompression assumes workgroup size equal to the subgroup size");

    // one workgroup decompresses one brick
    const uint workgroup_thread_id = gl_LocalInvocationID.x;
    const uint workgroup_size = gl_WorkGroupSize.x;

    const uint brick_idx = gl_WorkGroupID.x;
    if (isHelperLane(brick_idx, g_brick_count.x * g_brick_count.y * g_brick_count.z))
        return;

    const uvec3 brick = brick_idx2pos(brick_idx, g_brick_count);
    const uint brick_info_pos = brick_idx * 4u;

    // obtain requested inv_lod and previously assigned cache_idx
    const uint target_inv_lod = g_brick_info[brick_info_pos + BRICK_INFO_REQ_INV_LOD];
    const uint cache_idx = g_brick_info[brick_info_pos + BRICK_INFO_CACHE_INDEX];

    // barrier must be here before thread 0 resets the REQ_INV_LOD
    barrier();
    // reset the requested LOD. the renderer will have to flag everything visible again
    // no return (except the isHelperLane return) allowed before this point
    if (workgroup_thread_id == 0u) {
        g_brick_info[brick_info_pos + BRICK_INFO_REQ_INV_LOD] = max(LOD_COUNT, g_brick_info[brick_info_pos + BRICK_INFO_REQ_INV_LOD]);
    }

    // if this brick has a valid request slot, it requests decompression. Otherwise, it can be ignored.
    if (g_brick_info[brick_info_pos + BRICK_INFO_REQ_SLOT] == INVALID) {
        return;
    }
    assert(0 < target_inv_lod && target_inv_lod < LOD_COUNT, "brick has slot index but requests zero or invalid lod");

    // decompression requested for brick, but brick could not receive a free cache region
    if (cache_idx == INVALID) {
        if (workgroup_thread_id == 0u) {
            g_brick_info[brick_info_pos + BRICK_INFO_CUR_INV_LOD] = INVALID;
            g_brick_info[brick_info_pos + BRICK_INFO_REQ_SLOT] = INVALID;
        }
        return;
    }

    ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
    //                  perform the decompression of the brick into the previously assigned cache region              //
    ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
    #ifdef SEPARATE_DETAIL
        #error "subgroup parallelized decoding does not support detail separation"
    #endif


#ifndef NDEBUG
    if (workgroup_thread_id == 0u) {
        verifyBrickCompression(brick_idx);
    }
#endif

    EncodingRef global_brick_encoding = getBrickEncodingRef(brick_idx);
    const uint brick_encoding_length = getBrickEncodingLength(brick_idx);
#if ENCODING_MODE == HUFFMAN_WM_ENC
    WMHBrickHeaderRef global_wm_header = getWMHBrickHeaderFromEncoding(global_brick_encoding);
    BitVectorRef global_bit_vector = getWMHBitVectorFromEncoding(global_brick_encoding);
#endif

    // 1. copy brick encoding to shared memory
#ifdef DECODE_FROM_SHARED_MEMORY
    // TODO: care about detail encoding! should be glued to the back of the same region
    #if ENCODING_MODE == NIBBLE_ENC
        assert(brick_encoding_length <= MAX_BRICK_ENCODING_32BIT_LENGTH, "shared memory brick encoding buffer is too small");
        // TODO: copying one uvec4 with each thread is still coalescing and much faster!
        for (uint e = workgroup_thread_id; e < brick_encoding_length; e += workgroup_size) {
            s_brick_encoding[e] = global_brick_encoding.buf[e];
        }
    #elif ENCODING_MODE == HUFFMAN_WM_ENC
        // copy the wavelet matrix header to shared memory
        uint flat_rank_size =  getFlatRankEntries(global_wm_header.bit_vector_size);
        if (workgroup_thread_id == 0u) {
            s_wm_header[0].bit_vector_size = global_wm_header.bit_vector_size;
            s_stop_bits_ref = getWMHStopBitsFromEncoding(global_brick_encoding,
                                                         brick_encoding_length,
                                                         global_brick_encoding.buf[PALETTE_SIZE_HEADER_INDEX]);
        }
        if (workgroup_thread_id < 5u)
            s_wm_header[0].ones_before_level[workgroup_thread_id] = global_wm_header.ones_before_level[workgroup_thread_id];
        if (workgroup_thread_id < 4u)
            s_wm_header[0].level_starts_1_to_4[workgroup_thread_id] = global_wm_header.level_starts_1_to_4[workgroup_thread_id];
        assert(flat_rank_size <= MAX_FLAT_RANK_WORD_LENGTH, "shared memory too small to fit full flat rank");
        for (uint e = workgroup_thread_id; e < flat_rank_size; e += workgroup_size) {
            s_wm_header[0].fr[e] = global_wm_header.fr[e];
        }
        // copy the bit vector to shared memory
        uint bit_vector_word_size = (global_wm_header.bit_vector_size + BV_WORD_BIT_SIZE - 1u) / BV_WORD_BIT_SIZE;
        assert(bit_vector_word_size <= MAX_BIT_VECTOR_WORD_LENGTH, "shared memory too small to fit full bit vector");
        for (uint e = workgroup_thread_id; e < bit_vector_word_size; e += workgroup_size) {
            s_bit_vector[e] = global_bit_vector.words[e];
        }
    #endif

    memoryBarrierShared();
    barrier();
    // from here on, the respective decoder can access the valid brick encoding in shared memory
#endif
    // 2. iterate over output voxels of brick (cache positions) for the requested LOD and decompress each one in parallel
    const uint decoded_brick_start_uint = cache_idx * g_cache_base_element_uints;
    const uint output_voxel_count = 1u << (3u * target_inv_lod);
    const uint target_brick_size = 1u << target_inv_lod;
    // The workgroup_size many work items iterate through the Morton indexing order from front to back.
    // The threads work on the next following items in parallel.
    //
    // If workgroup_size >= number_of_output_voxels: each thread computes ONE output element
    // If workgroup_size <  number_of_output_voxels: each thread computes multiple output elements
    // With a workgroup_size equal to the subgroup size, optimization like vulkan subgroup operations are posssible.
    uint output_i = workgroup_thread_id;
    while (output_i < output_voxel_count) {
        #if ENCODING_MODE == NIBBLE_ENC
            decompressCSGVVoxelToCache(output_i, target_inv_lod, uvec3(BRICK_SIZE),
                        #ifndef DECODE_FROM_SHARED_MEMORY
                                global_brick_encoding,
                        #endif
                                brick_encoding_length, decoded_brick_start_uint);
        #elif ENCODING_MODE == HUFFMAN_WM_ENC
            decompressCSGVVoxelToCache(output_i, target_inv_lod, uvec3(BRICK_SIZE),
                                global_brick_encoding, brick_encoding_length,
                        #ifndef DECODE_FROM_SHARED_MEMORY
                                global_wm_header, global_bit_vector,
                        #endif
                                decoded_brick_start_uint);
        #endif
        output_i += workgroup_size;
    }

    ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

    // the brick was successfuly decompressed into the cache into the requested inverse LOD, the request can be removed.
    if (workgroup_thread_id == 0u) {
        g_brick_info[brick_info_pos + BRICK_INFO_CUR_INV_LOD] = target_inv_lod;
        g_brick_info[brick_info_pos + BRICK_INFO_REQ_SLOT] = INVALID;
    }

#endif // CACHE_MODE == CACHE_BRICKS
}
